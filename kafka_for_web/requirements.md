Kafka Cluster
    ├── Brokers (Server 1, Server 2, ...)
           ├── Partitions (for different topics)
           ├── Topic A - Partition 1 (Leader: Broker 1, Followers: Broker 2, Broker 3)
           ├── Topic A - Partition 2 (Leader: Broker 2, Followers: Broker 1, Broker 3)
           ├── Topic B - Partition 1 (Leader: Broker 3, Followers: Broker 1, Broker 2)
           └── ...

# Build it into a web API.
### In the case of this app, when a cluster is created, the 3 Brokers are automatically created.

Messages should be able to be sent in batch. The bottleneck is typically over network bandwidth.
Something called asynchronous send. Producers will wait to accumulate a certain amount of data before hitting send, to save network bandwidth.

The offset are regularly committed to an internal topic called __consumer_offsets 
The partition should keep track of the offset and save it to a log file  

Workflow -
When a producer produces a message, the message will be hashed into one of the given partitions; It will then be saved in the log file.
If none are given, then it will use **round-robin**
The producer then receives a message that the message has been commited and not lost. 
_______
Different partitions are used to ensure consistency and partition tolerance
	
# Api endpoints
### Producer 
Publishes a new event
 - /events - POST
Body 
{ 
    "topic": string
}
	
- /cluster - POST
Allow the user to create a new cluster from the API Route. 
Creates a new Cluster and a new directory inside the working directory. Initialises a default of 1 broker and no topics inside
{
    "name": string
    "num_brokers": number
    "log_directory": string
    - The number of partitions that we choose to keep
    "partitions": number
    - How long we choose to keep the logs for
    "retention_policies": number
}
 
- /topic - POST
Creates a new topic under the following cluster
{
    name: string
    cluster: string
}

Posting data to /events should persist in a log file. 

# Consumer
 - /events - GET 
Subscribes to a new event
Body { 
    "topic": "liked"
}
Subscribes to the liked topic.

 - /offset - POST  
 {
    consumer_id: Number
 }
 Commits the current offset of the consumer manually

In order to build a Kafka Clone, the software must have the following features:
 - Publisher/Consumer model 
 - Partition tolerance
 - Different topics that consumers can subscribe to
 - Saving data to a sequential log file
 - Off set management - Seeing where the consumer is up to when consuming
 - Allow for batch processing, and also single report processing
 - Message durability. Saving the log file to multiple locations

Maybe features
 - Different brokers within those topics
 - Data replication and sharding on local storage

Database Schema
 - Producer
class Producer {
	producer_id: number

}

Broker {

}

Topic {
	id: number;
	name: string;
	contents: MessageLog
}


 - Consumer
class Consumer {
	consumer_id: number
}

# Schemas 
## This is just a general idea of what the table should look like. Generated by ChatGPT. 
CREATE TABLE offset_commitments (
    id SERIAL PRIMARY KEY,
    consumer_group_id INTEGER NOT NULL,
    topic_id INTEGER NOT NULL,
    partition_number INTEGER NOT NULL,
    offset BIGINT NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (consumer_group_id) REFERENCES consumer_groups(id),
    FOREIGN KEY (topic_id) REFERENCES topics(id)
);


CREATE TABLE consumer_groups (
    id SERIAL PRIMARY KEY,
    group_name VARCHAR(255) NOT NULL,
    member_id VARCHAR(255) NOT NULL,
    topic_id INTEGER NOT NULL,
    partition_number INTEGER NOT NULL,
    offset BIGINT NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (topic_id) REFERENCES topics(id)
);

CREATE TABLE brokers (
    id SERIAL PRIMARY KEY,
    host VARCHAR(255) NOT NULL,
    port INTEGER NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE partitions (
    id SERIAL PRIMARY KEY,
    topic_id INTEGER NOT NULL,
    partition_number INTEGER NOT NULL,
    leader_broker_id INTEGER NOT NULL,
    replicas INTEGER[] NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (topic_id) REFERENCES topics(id)
);

CREATE TABLE producers {
    id SERIAL PRIMARY KEY,
    name NOT NULL 
}


