Kafka Cluster
    ├── Brokers (Server 1, Server 2, ...)
           ├── Partitions (for different topics)
           ├── Topic A - Partition 1 (Leader: Broker 1, Followers: Broker 2, Broker 3)
           ├── Topic A - Partition 2 (Leader: Broker 2, Followers: Broker 1, Broker 3)
           ├── Topic B - Partition 1 (Leader: Broker 3, Followers: Broker 1, Broker 2)
           └── ...

# Build it into a web API.
### In the case of this app, when a cluster is created, the 3 Brokers are automatically created.

Messages should be able to be sent in batch. The bottleneck is typically over network bandwidth.
Something called asynchronous send. Producers will wait to accumulate a certain amount of data before hitting send, to save network bandwidth.

The offset are regularly committed to an internal topic called __consumer_offsets 
The partition should keep track of the offset and save it to a log file  

Workflow -
When a producer produces a message, the message will be hashed into one of the given partitions; It will then be saved in the log file.
If none are given, then it will use **round-robin**
The producer then receives a message that the message has been commited and not lost. 
_______
Different partitions are used to ensure consistency and partition tolerance
	
# Api endpoints
### Producer 
Publishes a new event
 - /events - POST
Body 
{ 
    "topic": string
}
	
- /cluster - POST
Allow the user to create a new cluster from the API Route. 
Creates a new Cluster and a new directory inside the working directory. Initialises a default of 1 broker and no topics inside
{
    "name": string
    "num_brokers": number
    "log_directory": string
    - The number of partitions that we choose to keep
    "partitions": number
    - How long we choose to keep the logs for
    "retention_policies": number
}
 
- /topic - POST
Creates a new topic under the following cluster
{
    name: string
    cluster: string
}

Posting data to /events should persist in a log file. 

# Consumer
 - /events - GET 
Subscribes to a new event
Body { 
    "topic": "liked"
}
Subscribes to the liked topic.

 - /offset - POST  
 {
    consumer_id: Number
 }
 Commits the current offset of the consumer manually

In order to build a Kafka Clone, the software must have the following features:
 - Publisher/Consumer model 
 - Partition tolerance
 - Different topics that consumers can subscribe to
 - Saving data to a sequential log file
 - Off set management - Seeing where the consumer is up to when consuming
 - Allow for batch processing, and also single report processing
 - Message durability. Saving the log file to multiple locations

Maybe features
 - Different brokers within those topics
 - Data replication and sharding on local storage

Database Schema
 - Producer
class Producer {
	producer_id: number

}

Broker {

}

Topic {
	id: number;
	name: string;
	contents: MessageLog
}


 - Consumer
class Consumer {
	consumer_id: number
}

# Schemas 
## This is just a general idea of what the table should look like. Generated by ChatGPT. 
CREATE TABLE offset_commitments (
    id SERIAL PRIMARY KEY,
    consumer_group_id INTEGER NOT NULL,
    topic_id INTEGER NOT NULL,
    partition_number INTEGER NOT NULL,
    offset BIGINT NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (consumer_group_id) REFERENCES consumer_groups(id),
    FOREIGN KEY (topic_id) REFERENCES topics(id)
);


CREATE TABLE consumer_groups (
    id SERIAL PRIMARY KEY,
    group_name VARCHAR(255) NOT NULL,
    member_id VARCHAR(255) NOT NULL,
    topic_id INTEGER NOT NULL,
    partition_number INTEGER NOT NULL,
    offset BIGINT NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (topic_id) REFERENCES topics(id)
);

CREATE TABLE brokers (
    id SERIAL PRIMARY KEY,
    host VARCHAR(255) NOT NULL,
    port INTEGER NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);


CREATE TABLE partitions (
    id SERIAL PRIMARY KEY,
    topic_id INTEGER NOT NULL,
    partition_number INTEGER NOT NULL,
    leader_broker_id INTEGER NOT NULL,
    replicas INTEGER[] NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (topic_id) REFERENCES topics(id)
);

CREATE TABLE producers {
    id SERIAL PRIMARY KEY,
    name NOT NULL 
}


# LOG FORMAT for writing to a partition
[date] [time] [topic] [partition] [offset] [key] [value] [action]

# Plan 
How to implement long polling.
1. The client should establish a connection to the server. 
2. The server should keep the connection open until a message is available. The server should check every second for a new message.
3. If there is a new message, then this response should be forwarded to the client. If there is no message within the set interval, then the connection should time out. 
4. The client should then re-establish a new connection to the server after it is done processing the previous message. 

## How it actually works in code
1. Client sends a HTTP Request to the server. 
2. Server keeps the connection open for a set time of 5 second. Every second, check for new messages in the log file that the client is subscribed to.
3. Then increment the consumer offset stored in the __consumer_offset topic. 
4. The client then receives the message and processes it.
5. Repeat the process
 
- Depending on the user offset.
- current_offset < end-of-log offset, return the message at the next offset.
- if current_offset == end-of-log offest, then wait for new message to arrive for 5 seconds.
- If the user offset is greater than the end-of-log offset, then hold the connection open until there is a message.
